{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Girish D Bhatta\n",
    "#### Student ID: 29270863\n",
    "\n",
    "Date: 19/08/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "* json (for conversion of list of dictionaries to a json Object, included in Anaconda Python 3.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries. As part of this assessment task, 're' and 'json' libraries will be used for parsing the \n",
    "# input data and creating json and xml formats respectively.\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parse 29270863.dat File\n",
    "\n",
    "In order to begin the parsing process, we have to perform superficial analysis or preprocessing to understand the kind of headings/keys are present in the Job posting data provided in the aforementioned file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###     2.1. Reading the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read the contents of the file\n",
    "file_handler = open(\"29270863.dat\",\"r\")\n",
    "\n",
    "#storing the content in a variable for further processing\n",
    "input_text = file_handler.read()\n",
    "\n",
    "#closing the file once the reading is done.\n",
    "file_handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  2.2 Splitting the contents of the file\n",
    "\n",
    "On observation of \"29270863.dat\" file, it can be concluded that each job posting is separated by a string of '-' dashes. Therefore, splitting the contents of the file using '------------------------------' as a demiliter is a good way to begin with the parsing the data to required formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 29270863.dat has 30942 job postings.\n"
     ]
    }
   ],
   "source": [
    "# splitting the contents by the string of '-'.\n",
    "job_data = input_text.split('------------------------------')\n",
    "\n",
    "# Since each job is separted by a the demiliter, the resultant list will have an empty string appended to the list.\n",
    "# hence, we need to get rid of it.\n",
    "del job_data[len(job_data)-1]\n",
    "\n",
    "print(\"The file 29270863.dat has \" + str(len(job_data)) + \" job postings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analysing the keys/headings of the file.\n",
    "\n",
    "Before we dive deep into the parsing of the file, we need analyse the keys/headings of the file. This is necessary in order to facilitate proper processing of data and to make sure that there is no data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Upon close observation it can found out that every heading/tile of a job posting has a common pattern. Each heading <u> starts at the beginning of the line follwed by a ':', which essentially marks the end of the heading/title, and the data related to that field follows</u>. we capture these patterns in a regular expression.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has a total of 323367 keys/headings\n",
      "['ID:', 'PROCEDURE:', 'to:', 'LOCATIONS:', 'DEADLINES:', '_TTL:', 'ABOUT:', 'REQ_QUALS:', 'REMUNERATION/\\nJOB RESPONSIBILITIES:', 'START DATE:', 'REMUNERATION:', '_description:', 'ID:', 'JOB_PROC:', '_LOC:', 'DEAD_LINE:', 'TITLES:', 'ABOUT:', 'QUALIFICATION:', 'REMUNERATION/\\nJOB RESPONSIBILITIES:', 'START DATE:', 'remuneration:', 'DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOC:', 'deadline:', 'JOB_T:', 'COMPANYS_INFO:', 'REQ_QUALS:', 'JOB RESPONSIBILITIES:', 'START DATE:', 'salary:', 'DESCRIPTION:', 'ID:', 'PROCEDURE:', 'LOCATION:', 'deadline:', 'TITLES:', '_info:', 'REQUIRED QUALIFICATIONS:', 'REMUNERATION/\\nRESP:', 'DATES:', 'SALARY:', 'DESCRIPTION:', 'JOB DESCRIPTION:', 'ID:', 'procedures:', 'LOCATION:', 'DEAD_LINE:', 'JOB_T:', 'REQUIRED QUALIFICATIONS:', 'JOB_RESPS:', 'START DATE:', 'JOB DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOC:', 'DEADLINES:', 'JOB TITLE:', 'about_company:', 'QUALIFS:', 'JOB RESPONSIBILITIES:', 'START DATE:', 'REMUNERATION:', 'JOB DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOCS:', 'DEAD_LINE:', 'title:', 'REQUIRED QUALIFICATIONS:', 'start_date:', 'job_desc:', 'ID:', 'JOB_PROC:', '_LOCS:', 'DEAD_LINE:', 'JOB TITLE:', 'qualifications:', 'JOB_RESPS:', 'START_DA:', 'REMUNERATION:', 'job_desc:', 'ID:', 'JOB_PROC:', 'LOCATIONS:', 'deadline:', 'JOB TITLE:', 'ABOUT COMPANY:', 'QUALIFICATION:', 'JOB_RESPS:', 'START_DA:', 'REMUNERATION:', 'JOB_DESC:', 'ID:', 'PROCEDURES:', 'LOCATIONS:', 'APPLICATION_DEADL:', 'JOB TITLE:']\n",
      "\n",
      "\n",
      "The file has a total of 229 distinct keys \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ABOUT COMPANY:': 11473,\n",
       " 'ABOUT:': 4330,\n",
       " 'APPLICATION_DEADL:': 3985,\n",
       " 'APPLICATION_DL:': 4373,\n",
       " 'COMPANYS_INFO:': 5810,\n",
       " 'DATES:': 3863,\n",
       " 'DATE_START:': 3284,\n",
       " 'DEADLINES:': 5694,\n",
       " 'DEAD_LINE:': 11564,\n",
       " 'DESCRIPTION:': 5778,\n",
       " 'EDUCATION/\\nJOB RESPONSIBILITIES:': 1,\n",
       " 'ID:': 30942,\n",
       " 'JOB DESCRIPTION:': 11740,\n",
       " 'JOB RESPONSIBILITIES:': 6178,\n",
       " 'JOB TITLE:': 11662,\n",
       " 'JOB_DESC:': 4053,\n",
       " 'JOB_LOC:': 4121,\n",
       " 'JOB_PROC:': 4295,\n",
       " 'JOB_PROCS:': 4055,\n",
       " 'JOB_RESPS:': 4350,\n",
       " 'JOB_SAL:': 3971,\n",
       " 'JOB_T:': 4016,\n",
       " 'LOCATION:': 11390,\n",
       " 'LOCATIONS:': 5741,\n",
       " 'PROCEDURE:': 11545,\n",
       " 'PROCEDURES:': 5655,\n",
       " 'QUALIFICATION:': 5827,\n",
       " 'QUALIFS:': 4107,\n",
       " 'REMUNERATION/\\nDATES:': 119,\n",
       " 'REMUNERATION/\\nDESCRIPTION:': 2,\n",
       " 'REMUNERATION/\\nJOB DESCRIPTION:': 3,\n",
       " 'REMUNERATION/\\nJOB RESPONSIBILITIES:': 5342,\n",
       " 'REMUNERATION/\\nREMUNERATION:': 16,\n",
       " 'REMUNERATION/\\nRESP:': 1834,\n",
       " 'REMUNERATION/\\nRESPONSIBILITY:': 2652,\n",
       " 'REMUNERATION/\\nSALARY:': 27,\n",
       " 'REMUNERATION/\\nSTART DATE:': 382,\n",
       " 'REMUNERATION:': 5794,\n",
       " 'REQUIRED QUALIFICATIONS:': 11295,\n",
       " 'REQ_QUALS:': 4434,\n",
       " 'RESP:': 2131,\n",
       " 'RESPONSIBILITY:': 3058,\n",
       " 'SALARY:': 11537,\n",
       " 'SKILLS/\\nRESPONSIBILITY:': 1,\n",
       " 'START DATE:': 11176,\n",
       " 'START_DA:': 5682,\n",
       " 'TITLES:': 5651,\n",
       " '_LOC:': 4424,\n",
       " '_LOCS:': 3113,\n",
       " '_TTL:': 3172,\n",
       " '_description:': 4277,\n",
       " '_info:': 3219,\n",
       " 'about_company:': 3945,\n",
       " 'activities:': 5,\n",
       " 'address:': 146,\n",
       " 'addressed:': 3,\n",
       " 'addresses:': 10,\n",
       " 'administration:': 1,\n",
       " 'advertising:': 2,\n",
       " 'analysis:': 4,\n",
       " 'and:': 4,\n",
       " 'andhttp:': 13,\n",
       " 'applications:': 16,\n",
       " 'apply:': 1,\n",
       " 'are:': 5,\n",
       " 'areas:': 32,\n",
       " 'as:': 3,\n",
       " 'assignment:': 1,\n",
       " 'at:': 164,\n",
       " 'attention:': 2,\n",
       " 'backend:': 1,\n",
       " 'below:': 2,\n",
       " 'by:': 11,\n",
       " 'call:': 30,\n",
       " 'campaign:': 2,\n",
       " 'characteristics:': 5,\n",
       " 'competencies:': 1,\n",
       " 'components:': 11,\n",
       " 'confidentiality:': 4,\n",
       " 'considerations:': 1,\n",
       " 'content:': 1,\n",
       " 'convention:': 1,\n",
       " 'countries:': 2,\n",
       " 'cover:': 1,\n",
       " 'criteria:': 15,\n",
       " 'dashboards:': 3,\n",
       " 'deadline:': 3099,\n",
       " 'deliverables:': 5,\n",
       " 'demonstrate:': 2,\n",
       " 'departments:': 2,\n",
       " 'desirable:': 2,\n",
       " 'details:': 1,\n",
       " 'development:': 3,\n",
       " 'dialogue:': 2,\n",
       " 'direction:': 1,\n",
       " 'distributors:': 3,\n",
       " 'documentation:': 6,\n",
       " 'documents:': 25,\n",
       " 'duties:': 3,\n",
       " 'education:': 1,\n",
       " 'elements:': 1,\n",
       " 'email:': 14,\n",
       " 'encompass:': 1,\n",
       " 'envisaged:': 1,\n",
       " 'essential:': 3,\n",
       " 'extentions:': 3,\n",
       " 'fax:': 24,\n",
       " 'features:': 2,\n",
       " 'fields:': 1,\n",
       " 'following:': 260,\n",
       " 'follows:': 18,\n",
       " 'for:': 6,\n",
       " 'form:': 5,\n",
       " 'functions:': 1,\n",
       " 'fund:': 26,\n",
       " 'groups:': 1,\n",
       " 'http:': 183,\n",
       " 'https:': 19,\n",
       " 'ideas:': 2,\n",
       " 'implementation:': 11,\n",
       " 'include:': 12,\n",
       " 'includes:': 4,\n",
       " 'including:': 63,\n",
       " 'inclusive:': 3,\n",
       " 'incumbent:': 1,\n",
       " 'indicators:': 2,\n",
       " 'industries:': 4,\n",
       " 'information:': 9,\n",
       " 'institution:': 2,\n",
       " 'interest:': 2,\n",
       " 'investors:': 3,\n",
       " 'issued:': 1,\n",
       " 'job_desc:': 3268,\n",
       " 'language:': 1,\n",
       " 'languages:': 3,\n",
       " 'letter:': 6,\n",
       " 'limitation:': 3,\n",
       " 'limited:': 4,\n",
       " 'line:': 11,\n",
       " 'lines:': 5,\n",
       " 'link:': 1,\n",
       " 'mail:': 3,\n",
       " 'matters:': 2,\n",
       " 'message:': 11,\n",
       " 'methodologies:': 7,\n",
       " 'ministries:': 3,\n",
       " 'modules:': 1,\n",
       " 'must:': 3,\n",
       " 'network:': 1,\n",
       " 'number:': 7,\n",
       " 'numbers:': 1,\n",
       " 'of:': 9,\n",
       " 'offered:': 4,\n",
       " 'office:': 5,\n",
       " 'on:': 8,\n",
       " 'organizations:': 3,\n",
       " 'outcomes:': 1,\n",
       " 'outputs:': 4,\n",
       " 'packages:': 30,\n",
       " 'parameterization:': 4,\n",
       " 'participants:': 1,\n",
       " 'particular:': 2,\n",
       " 'particularly:': 4,\n",
       " 'person:': 4,\n",
       " 'phone:': 6,\n",
       " 'phones:': 2,\n",
       " 'phytobenthos:': 3,\n",
       " 'phytoplankton:': 2,\n",
       " 'pillar:': 2,\n",
       " 'place:': 3,\n",
       " 'platforms:': 6,\n",
       " 'position:': 8,\n",
       " 'practices:': 1,\n",
       " 'procedure:': 1,\n",
       " 'procedures:': 3304,\n",
       " 'process:': 4,\n",
       " 'projects:': 2,\n",
       " 'qualifications:': 3186,\n",
       " 'qualified:': 5,\n",
       " 'questions:': 1,\n",
       " 'receive:': 6,\n",
       " 'remuneration:': 4325,\n",
       " 'representations:': 65,\n",
       " 'resources:': 4,\n",
       " 'responsibilities:': 3182,\n",
       " 'results:': 14,\n",
       " 'resume:': 5,\n",
       " 'review:': 2,\n",
       " 'salary:': 3222,\n",
       " 'sections:': 2,\n",
       " 'sectors:': 7,\n",
       " 'seminar:': 2,\n",
       " 'should:': 3,\n",
       " 'site:': 24,\n",
       " 'skills:': 2,\n",
       " 'specified:': 1,\n",
       " 'stages:': 11,\n",
       " 'stakeholders:': 1,\n",
       " 'start_date:': 4270,\n",
       " 'structures:': 5,\n",
       " 'subcomponents:': 1,\n",
       " 'subject:': 9,\n",
       " 'submission:': 1,\n",
       " 'subtopics:': 6,\n",
       " 'system:': 3,\n",
       " 'systems:': 1,\n",
       " 'tasks:': 28,\n",
       " 'team:': 4,\n",
       " 'technologies:': 7,\n",
       " 'tel:': 8,\n",
       " 'things:': 4,\n",
       " 'through:': 155,\n",
       " 'title:': 4333,\n",
       " 'to:': 2046,\n",
       " 'tools:': 7,\n",
       " 'topic:': 4,\n",
       " 'topics:': 6,\n",
       " 'trade:': 2,\n",
       " 'training:': 2,\n",
       " 'tranches:': 3,\n",
       " 'using:': 4,\n",
       " 'value:': 1,\n",
       " 'via:': 2,\n",
       " 'visit:': 107,\n",
       " 'volume:': 1,\n",
       " 'website:': 73,\n",
       " 'will:': 1,\n",
       " 'with:': 2,\n",
       " 'write:': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input_keys will have a list of all the keys/fields. The regular expression captures all keys. i.e, keys with all\n",
    "# upper case characters, upper case characters beginning with a '_', upper case characters separated by '/' etc. similarly\n",
    "# for all the lower case character and subsequent combinations as well.\n",
    "input_keys = re.findall(r\"(^_?[A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*:|^_[a-z]+:|^[a-z]+_?[a-z]*:)\",input_text,re.MULTILINE)\n",
    "print(\"The file has a total of \" + str(len(input_keys)) + \" keys/headings\") \n",
    "\n",
    "#printing the first 100 keys \n",
    "print(input_keys[0:100])\n",
    "\n",
    "# code snippet to count the number of occurrences of each of the obtained keys\n",
    "key_dict = {}\n",
    "for each_key in input_keys:\n",
    "    if each_key not in key_dict:\n",
    "        key_dict[each_key] = 1\n",
    "    else:\n",
    "        key_dict[each_key] += 1\n",
    "\n",
    "print(\"\\n\\nThe file has a total of \" + str(len(key_dict)) + \" distinct keys \")\n",
    "key_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the regular expression seems to be working fine extracting all the required fields for us, but it also is capturing the data with similar pattern as part of the value. values like 'to:','address:','and:','are:' etc. Therefore we need to refine our regular expression further, which will only extract the required fields and not unnecessary data which appears as part of the value data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has a total of 319279 keys/headings\n",
      "['ID:', 'PROCEDURE:', 'LOCATIONS:', 'DEADLINES:', '_TTL:', 'ABOUT:', 'REQ_QUALS:', 'REMUNERATION/\\nJOB RESPONSIBILITIES:', 'START DATE:', 'REMUNERATION:', '_description:', 'ID:', 'JOB_PROC:', '_LOC:', 'DEAD_LINE:', 'TITLES:', 'ABOUT:', 'QUALIFICATION:', 'REMUNERATION/\\nJOB RESPONSIBILITIES:', 'START DATE:', 'remuneration:', 'DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOC:', 'deadline:', 'JOB_T:', 'COMPANYS_INFO:', 'REQ_QUALS:', 'JOB RESPONSIBILITIES:', 'START DATE:', 'salary:', 'DESCRIPTION:', 'ID:', 'PROCEDURE:', 'LOCATION:', 'deadline:', 'TITLES:', '_info:', 'REQUIRED QUALIFICATIONS:', 'REMUNERATION/\\nRESP:', 'DATES:', 'SALARY:', 'DESCRIPTION:', 'JOB DESCRIPTION:', 'ID:', 'procedures:', 'LOCATION:', 'DEAD_LINE:', 'JOB_T:', 'REQUIRED QUALIFICATIONS:', 'JOB_RESPS:', 'START DATE:', 'JOB DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOC:', 'DEADLINES:', 'JOB TITLE:', 'about_company:', 'QUALIFS:', 'JOB RESPONSIBILITIES:', 'START DATE:', 'REMUNERATION:', 'JOB DESCRIPTION:', 'ID:', 'PROCEDURES:', '_LOCS:', 'DEAD_LINE:', 'title:', 'REQUIRED QUALIFICATIONS:', 'start_date:', 'job_desc:', 'ID:', 'JOB_PROC:', '_LOCS:', 'DEAD_LINE:', 'JOB TITLE:', 'qualifications:', 'JOB_RESPS:', 'START_DA:', 'REMUNERATION:', 'job_desc:', 'ID:', 'JOB_PROC:', 'LOCATIONS:', 'deadline:', 'JOB TITLE:', 'ABOUT COMPANY:', 'QUALIFICATION:', 'JOB_RESPS:', 'START_DA:', 'REMUNERATION:', 'JOB_DESC:', 'ID:', 'PROCEDURES:', 'LOCATIONS:', 'APPLICATION_DEADL:', 'JOB TITLE:', '_info:']\n",
      "\n",
      "\n",
      "The file has a total of 63 distinct keys \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ABOUT COMPANY:': 11473,\n",
       " 'ABOUT:': 4330,\n",
       " 'APPLICATION_DEADL:': 3985,\n",
       " 'APPLICATION_DL:': 4373,\n",
       " 'COMPANYS_INFO:': 5810,\n",
       " 'DATES:': 3863,\n",
       " 'DATE_START:': 3284,\n",
       " 'DEADLINES:': 5694,\n",
       " 'DEAD_LINE:': 11564,\n",
       " 'DESCRIPTION:': 5778,\n",
       " 'EDUCATION/\\nJOB RESPONSIBILITIES:': 1,\n",
       " 'ID:': 30942,\n",
       " 'JOB DESCRIPTION:': 11740,\n",
       " 'JOB RESPONSIBILITIES:': 6178,\n",
       " 'JOB TITLE:': 11662,\n",
       " 'JOB_DESC:': 4053,\n",
       " 'JOB_LOC:': 4121,\n",
       " 'JOB_PROC:': 4295,\n",
       " 'JOB_PROCS:': 4055,\n",
       " 'JOB_RESPS:': 4350,\n",
       " 'JOB_SAL:': 3971,\n",
       " 'JOB_T:': 4016,\n",
       " 'LOCATION:': 11390,\n",
       " 'LOCATIONS:': 5741,\n",
       " 'PROCEDURE:': 11545,\n",
       " 'PROCEDURES:': 5655,\n",
       " 'QUALIFICATION:': 5827,\n",
       " 'QUALIFS:': 4107,\n",
       " 'REMUNERATION/\\nDATES:': 119,\n",
       " 'REMUNERATION/\\nDESCRIPTION:': 2,\n",
       " 'REMUNERATION/\\nJOB DESCRIPTION:': 3,\n",
       " 'REMUNERATION/\\nJOB RESPONSIBILITIES:': 5342,\n",
       " 'REMUNERATION/\\nREMUNERATION:': 16,\n",
       " 'REMUNERATION/\\nRESP:': 1834,\n",
       " 'REMUNERATION/\\nRESPONSIBILITY:': 2652,\n",
       " 'REMUNERATION/\\nSALARY:': 27,\n",
       " 'REMUNERATION/\\nSTART DATE:': 382,\n",
       " 'REMUNERATION:': 5794,\n",
       " 'REQUIRED QUALIFICATIONS:': 11295,\n",
       " 'REQ_QUALS:': 4434,\n",
       " 'RESP:': 2131,\n",
       " 'RESPONSIBILITY:': 3058,\n",
       " 'SALARY:': 11537,\n",
       " 'SKILLS/\\nRESPONSIBILITY:': 1,\n",
       " 'START DATE:': 11176,\n",
       " 'START_DA:': 5682,\n",
       " 'TITLES:': 5651,\n",
       " '_LOC:': 4424,\n",
       " '_LOCS:': 3113,\n",
       " '_TTL:': 3172,\n",
       " '_description:': 4277,\n",
       " '_info:': 3219,\n",
       " 'about_company:': 3945,\n",
       " 'deadline:': 3099,\n",
       " 'job_desc:': 3268,\n",
       " 'procedure:': 1,\n",
       " 'procedures:': 3304,\n",
       " 'qualifications:': 3186,\n",
       " 'remuneration:': 4325,\n",
       " 'responsibilities:': 3182,\n",
       " 'salary:': 3222,\n",
       " 'start_date:': 4270,\n",
       " 'title:': 4333}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the refined regular expression will take care of all the fileds that appear in lower case and have the same pattern as that of\n",
    "# fields. we need to perform all the steps performed previosuly to check if all the necessary keys are getting captured.\n",
    "\n",
    "res = re.findall(r\"(^_?[A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*:|^_[a-z]+:|^[a-z]+_[a-z]*:|^salary:|^titles?:|^locations?:|^remuneration:|^descriptions?:|^responsibility:|^responsibilities:|^qualifications?:|^procedures?:|^deadline:|^about:)\"\n",
    "                 ,input_text,re.MULTILINE)\n",
    "\n",
    "print(\"The file has a total of \" + str(len(res)) + \" keys/headings\") \n",
    "\n",
    "#printing the first 100 keys \n",
    "print(res[0:100])\n",
    "\n",
    "# code snippet to count the number of occurrences of each of the obtained keys\n",
    "key_dict = {}\n",
    "for each_key in res:\n",
    "    if each_key not in key_dict:\n",
    "        key_dict[each_key] = 1\n",
    "    else:\n",
    "        key_dict[each_key] += 1\n",
    "\n",
    "print(\"\\n\\nThe file has a total of \" + str(len(key_dict)) + \" distinct keys \")\n",
    "key_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can think that the refinement has yielded a fair result cutting down the number of keys from 229 to 63 keys. Further, we need to standardise the names of the fields by considering the nmeanings they convey and replacing the variants of a key/field with a standard key/field value. For example 'ABOUT COMPANY','ABOUT','_info','about_company' all mean the same and hence standarization is necessary for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dissection/Explanation of regex to extract keys/fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE now Dissect our regular expression to capture keys/Fields of each document.\n",
    "\n",
    "The regular expression we have used is : \n",
    "\n",
    "<b>(^_?[A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*:|^_[a-z]+:|^[a-z]+_[a-z]*:|^salary:|^titles?:|^locations?:|^remuneration:|^descriptions?:|^responsibility:|^responsibilities:|^qualifications?:|^procedures?:|^deadline:|^about:)</b>\n",
    "\n",
    "The regular expression has 3 major sections:\n",
    "\n",
    "1. <b>^_?[A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*:</b> ----> This part of regular expression handles all the keys which are uppercase. We have seen keys like _TTL which begin with '_' hence we begine with ^_?. the ^ caret ensures that this pattern is captured at the beginning of the string. [A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*: This part matches all the keys which are in uppercase followed by a space or a tab captured by \\s. followed by another underscore '_' or '/' character which is captured by [_/]* this part. this can be followed by any number of uppercase characters which is captured by *\\s*[A-Z]*:. It is important to notice that every regex to capture fields/keys should end with a colon ':' this will serve as a delimiter to capture keys/fields.\n",
    "    \n",
    "2. <b>^_[a-z]+:|^[a-z]+_[a-z]*:</b> ----> This part of regex matches all the keys which have lower case characters. these are of two types. The ones which like _loc which represent the location of the job posting start with an underscore. Hence  ^_[a-z]+: this regex matches all the keys which start with the patter of '_' followed by 1 or more lower case characters and ending with a colon. ^[a-z]+_[a-z]*: this regex checks for keys with two words joined by a '_' between them. this will capture keys like \"job_t\".\n",
    "    \n",
    "3. <b>^salary:|^titles?:|^locations?:|^remuneration:|^descriptions?:|^responsibility:|\n",
    "    responsibilities:|^qualifications?:|^procedures?:|^deadline:|^about:</b> ----> This regex is used to eliminate indesirable tokens like \"'to:','address:','and:','are:'\". Although this regex looks quiet absurd, during our prelimiary analysis, we had observed that extraction of undesirable tokens occured only with lower case keys/fields. As we do know that the keys we need to extract will belong to the aforementioned fields, we just looks for them in the regex, thereby, eliminating unnecessary extraction of undesirable tokens like \"to:\",\"with:\",\"address:\" etc. The reason for using only these specific strings as a part of our regex can be attributed to the preliminary analysis of the number of occurences of these keys. we have used those keys which appear most of the times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Standardization of Keys and extraction of data\n",
    "\n",
    "We now start to standardize all the keys of the of the split content, extract the data for the all the fields and persist the extracted data in a dictionary and append it to a list. that means, each dictioanry will represent each job posting and a list will have a collection of job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of jobs are 30942\n",
      "[{'_id': ' 94582\\n', 'application_procedure': ' To apply for this position, please submit a\\ndetailed resume/CV and a cover letter in Armenian and English languages\\nto: Haypost CJSC HR Department (6th floor, 22 Saryan Str., 0002 Yerevan,\\nRepublic of Armenia) or e-mail those to: HRManager@.... \\nPlease, mention in subject line of your e-mail the position title. \\nOnly short listed candidates will be notified for the interview.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.\\n', 'location': ' Yerevan, Armenia\\n', 'application_deadline': ' 20 May 2008\\n', 'title': ' Marketing Specialist\\n', 'about_company': '\\n \"\"Orange Fitness and Tennis Club\"\" is a fitness club\\noffering its customers a bundle of services directed to their health\\nimprovement and active leisure.\\n', 'required_qualifications': '\\n - University degree in Law or Economics;\\n- At least 3 years of diversified legal experience, including at least 2\\nyears of operational experience in the Urban development sector and\\nspecialized subject area;\\n- Proven knowledge of Urban development sector regulations in Armenia;\\n- Previous experience in conducting needs assessments may be an asset;  \\n- Excellent research, communication and presentation skills;\\n- Experience in similar consulting(s) for international agencies is\\ndesirable.\\n', 'job_responsibilities': '\\n - Receive visitors and handle telephone calls to the office staff;\\n- Screen and sort mail in order of priority for your manager/other staff\\nmembers;\\n- Prepare and type reports, letters and memoranda, making necessary\\ntranslations as instructed;\\n- Make all necessary travel arrangements, including hotel reservations\\nand visas;\\n- Process expense claims, keeping necessary records;\\n- Arrange internal and external meetings and assist in the organisation\\nof workshops and seminars;\\n- Maintain diaries and schedules, liasing with the Head of Office as\\nnecessary;\\n- Develop and maintain a comprehensive filing system for the Office to\\nensure instant retrieval of up-to-date information (in consultation with\\nOffice Manager);\\n- Assist on ad hoc administrative duties as required.\\n', 'start_date': ' 01 April 2009\\n', 'salary': ' Competitive\\n', 'job_descriptions': ' The incumbent will be responsible for\\nbusiness-to-business sales in Yerevan.'}, {'_id': ' 92506\\n', 'application_procedure': ' If you meet the requirements above and are\\nconfident that your background and experience qualify you for the\\nposition, please e-mail your resume to: career@... , mentioning the\\ntitle of the announced position \"\"Fiber Optic Network Technician\"\" in the\\nsubject line.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.\\n', 'location': ' Yerevan, Armenia\\n', 'application_deadline': ' 30 April 2007\\n', 'title': ' Deputy Chief  Accountant\\n', 'about_company': \"\\n UNDP is the UN's global development network, advocating\\nfor change and connecting countries to knowledge, experience and\\nresources to help people build a better life.\\n\", 'required_qualifications': '\\n - At least 3 years of work experience in Armenian insurance market as a\\nMarketing Executive;\\n- Good command of Armenian, Russian and English languages;\\n- Good computer skills (MS Word and Excel);\\n- Excellent interpersonal, communication and co-operation skills.\\n', 'job_responsibilities': '\\n - Design mechanical and automatic and semi automatic machinery;\\n- Perform contract job from own location, such as house, office etc.\\n', 'start_date': ' 15 August 2012\\n', 'salary': ' Competitive salary\\n', 'job_descriptions': ' Medical Representative will be responsible for\\ncreating the awareness of GMPharmaceuticals products among doctors,\\npharmacists and general population.'}]\n"
     ]
    }
   ],
   "source": [
    "# A list that will contain all the job postings,basically it will be a list of dictionaries, where each dictionary represents a\n",
    "# job posting.\n",
    "all_jobs = []\n",
    "\n",
    "# This code snippet iterates over the job_data list, where each list item is a job posting, it creates dictionary for each list\n",
    "# list item, extracts all the fields, creates a dynamic regex to extract all the values and then store it in the dictionary with\n",
    "# appropriate key/field names and lastly append the dictionary to a list.\n",
    "for job in job_data:\n",
    "   count = 0 # to keep track of the keys/fileds of the job posting\n",
    "   job = job.strip() # to get rid of extra white spaces in the text.\n",
    "   each_job = {} #dictionary to capture each job posting's data\n",
    "   res = re.findall(r\"(^_?[A-Z]+\\s?[_/]*\\s*[A-Z]*\\s*[A-Z]*:|^_[a-z]+:|^[a-z]+_[a-z]*:|^salary:|^titles?:|^locations?:|^remuneration:|^descriptions?:|^responsibility:|^responsibilities:|^qualifications?:|^procedures?:|^deadline:|^about:)\"\n",
    "                 ,job,re.MULTILINE) # regex to capture all the keys/fields of the job posting\n",
    "    \n",
    "   # loop to iterate over the list of keys,look for patterns, standardize the keys and extract the associated value \n",
    "   for i in range(len(res)):\n",
    "        count += 1        \n",
    "        if count == len(res): # if the the key is the last element of the list\n",
    "            my_regex = res[i]+r\"(.*)\" #dyanmic regular expression that with a capturing group, \n",
    "                                    #that starts with the key and is not bounded by a ending key.\n",
    "            \n",
    "            # the keys are standardized in accordance to the keys mentioned in the sample.xml file   \n",
    "            if re.search(r\"(ID:)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"_id\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*loc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"location\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*desc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"job_descriptions\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*resp.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"job_responsibilities\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*qual.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"required_qualifications\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*salary.*|.*remuneration.*|job_sal)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"salary\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*proc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"application_procedure\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*dead.*|application_dl)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"application_deadline\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*title.*|.*ttl.*|job_t)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"title\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*about.*|.*info.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"about_company\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*start.*|.*date.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"start_date\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "        else:    \n",
    "            my_regex = res[i]+r\"(.*?)\"+res[i+1] #dynamic regex that starts with a given key pointed by res[i], followed by\n",
    "                                                # a capturing group, to capture the data of that field, until it reaches the key\n",
    "                                                # pointed by res[i+1]\n",
    "            \n",
    "            if re.search(r\"(ID:)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"_id\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*loc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"location\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*desc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"job_descriptions\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*resp.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"job_responsibilities\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*qual.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"required_qualifications\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*salary.*|.*remuneration.*|job_sal)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"salary\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*proc.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"application_procedure\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*dead.*|application_dl)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"application_deadline\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*title.*|.*ttl.*|job_t)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"title\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*about.*|.*info.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"about_company\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "            elif re.search(r\"(.*start.*|.*date.*)\",res[i],re.IGNORECASE):\n",
    "                each_job[\"start_date\"] = re.search(my_regex,job,re.DOTALL).group(1)\n",
    "   all_jobs.append(each_job)\n",
    "\n",
    "print(\"The total number of jobs are \" + str(len(all_jobs)))\n",
    "\n",
    "# printing the first 2 jobs\n",
    "print(all_jobs[0:2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain keys like 'job_responsibilities','job_descriptions','required_qualifications' etc which have bullet points. these bullet points need to converted to list and and assigned back to a dictionary whose key will be the actual field names. This is necessary from the point of view of conversion to json and xml formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': '94582', 'application_procedure': 'To apply for this position, please submit a\\ndetailed resume/CV and a cover letter in Armenian and English languages\\nto: Haypost CJSC HR Department (6th floor, 22 Saryan Str., 0002 Yerevan,\\nRepublic of Armenia) or e-mail those to: HRManager@.... \\nPlease, mention in subject line of your e-mail the position title. \\nOnly short listed candidates will be notified for the interview.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.', 'location': 'Yerevan, Armenia', 'application_deadline': '20 May 2008', 'title': 'Marketing Specialist', 'about_company': '\"\"Orange Fitness and Tennis Club\"\" is a fitness club\\noffering its customers a bundle of services directed to their health\\nimprovement and active leisure.', 'required_qualifications': {'qualification': ['University degree in Law or Economics', '\\n At least 3 years of diversified legal experience, including at least 2\\nyears of operational experience in the Urban development sector and\\nspecialized subject area', '\\n Proven knowledge of Urban development sector regulations in Armenia', '\\n Previous experience in conducting needs assessments may be an asset', '  \\n Excellent research, communication and presentation skills', '\\n Experience in similar consulting(s) for international agencies is\\ndesirable.']}, 'job_responsibilities': {'responsibility': ['Receive visitors and handle telephone calls to the office staff', '\\n Screen and sort mail in order of priority for your manager/other staff\\nmembers', '\\n Prepare and type reports, letters and memoranda, making necessary\\ntranslations as instructed', '\\n Make all necessary travel arrangements, including hotel reservations\\nand visas', '\\n Process expense claims, keeping necessary records', '\\n Arrange internal and external meetings and assist in the organisation\\nof workshops and seminars', '\\n Maintain diaries and schedules, liasing with the Head of Office as\\nnecessary', '\\n Develop and maintain a comprehensive filing system for the Office to\\nensure instant retrieval of uptodate information (in consultation with\\nOffice Manager)', '\\n Assist on ad hoc administrative duties as required.']}, 'start_date': '01 April 2009', 'salary': 'Competitive', 'job_descriptions': 'The incumbent will be responsible for\\nbusiness-to-business sales in Yerevan.'}, {'_id': '92506', 'application_procedure': 'If you meet the requirements above and are\\nconfident that your background and experience qualify you for the\\nposition, please e-mail your resume to: career@... , mentioning the\\ntitle of the announced position \"\"Fiber Optic Network Technician\"\" in the\\nsubject line.\\nPlease clearly mention in your application letter that you learned of\\nthis job opportunity through Career Center and mention the URL of its\\nwebsite - www.careercenter.am, Thanks.', 'location': 'Yerevan, Armenia', 'application_deadline': '30 April 2007', 'title': 'Deputy Chief  Accountant', 'about_company': \"UNDP is the UN's global development network, advocating\\nfor change and connecting countries to knowledge, experience and\\nresources to help people build a better life.\", 'required_qualifications': {'qualification': ['At least 3 years of work experience in Armenian insurance market as a\\nMarketing Executive', '\\n Good command of Armenian, Russian and English languages', '\\n Good computer skills (MS Word and Excel)', '\\n Excellent interpersonal, communication and cooperation skills.']}, 'job_responsibilities': {'responsibility': ['Design mechanical and automatic and semi automatic machinery', '\\n Perform contract job from own location, such as house, office etc.']}, 'start_date': '15 August 2012', 'salary': 'Competitive salary', 'job_descriptions': 'Medical Representative will be responsible for\\ncreating the awareness of GMPharmaceuticals products among doctors,\\npharmacists and general population.'}]\n"
     ]
    }
   ],
   "source": [
    "# This code snippet converts the bullet points, which begin with a '-' and intermediately terminated by ';' and permanently\n",
    "# terminated by '.' into a list from a textual format. it looks for patterns in the values, and if such a pattern does exist, it \n",
    "# performs the necessary operation of splitting them by ';' as a delimiter and assigning it back to a dictionary whose key is \n",
    "# the field name itself.\n",
    "\n",
    "for each_obj in all_jobs:\n",
    "    for k in each_obj: \n",
    "        each_obj[k] = each_obj[k].strip()\n",
    "        if re.search(r'^-.*;$|^-.*\\.$',each_obj[k],re.DOTALL|re.MULTILINE): #looks for a pattern which has a '-' at the beginning\n",
    "                                                                            #and either ends with a ';' or '.'\n",
    "            obj = {} #dictionary to hold the data in key value pair, where value would be a list and key woould be the field name\n",
    "                     #from the parent dictionary.\n",
    "            if k == \"job_responsibilities\":\n",
    "                obj[\"responsibility\"] = each_obj[k].replace('-','').strip().split(';') #replacing the '-', stripping the whitespaces and splitting by\n",
    "                                                                                       #delimiter ';'\n",
    "                each_obj[k] = obj\n",
    "            elif k == \"job_descriptions\":\n",
    "                obj[\"description\"] = each_obj[k].replace('-','').strip().split(';')\n",
    "                each_obj[k] = obj\n",
    "            elif k == \"required_qualifications\":\n",
    "                obj[\"qualification\"] = each_obj[k].replace('-','').strip().split(';')\n",
    "                each_obj[k] = obj\n",
    "\n",
    "#Printing the first 2 job postings\n",
    "print(all_jobs[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Post standardization and extraction, we can analyse the number of occurences of the keys/fields, which would give insights into the presence/absence of these fields across job postings.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the distinct key and the number of their occurrences are :\n",
      "_id  :  30942\n",
      "application_procedure  :  28845\n",
      "location  :  28789\n",
      "application_deadline  :  28715\n",
      "title  :  28834\n",
      "about_company  :  28777\n",
      "required_qualifications  :  28839\n",
      "job_responsibilities  :  28722\n",
      "start_date  :  28275\n",
      "salary  :  28926\n",
      "job_descriptions  :  28821\n"
     ]
    }
   ],
   "source": [
    "# this code snippet collects all the keys/fields of all the job postings and calculates the number of occurences of each of these\n",
    "# keys. this would provide insights into the presences/absence of these fields across job postings\n",
    "\n",
    "keys_list = []\n",
    "for each_job in all_jobs:\n",
    "    for each_key in each_job:\n",
    "        keys_list.append(each_key)\n",
    "\n",
    "key_dict = {}\n",
    "for each_key in keys_list:\n",
    "    if each_key not in key_dict:\n",
    "        key_dict[each_key] = 1\n",
    "    else:\n",
    "        key_dict[each_key] += 1\n",
    " \n",
    "print(\"All the distinct key and the number of their occurrences are :\")\n",
    "for k,v in key_dict.items():\n",
    "    print(k,\" : \",v)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that except the key \"id\" none of the keys appear accross all the job postings. On an average we can see that out of 30942 job postings all other keys are missing in 1500 files. We can therfore conclude that not all keys/fields appear in all job postings except the key \"_id\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dumping the extracted data to a json file\n",
    "\n",
    "In this step we have to dump the extracted data to a json file. This can be done by making use of the 'dump' method by json library that was imported earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The json content was written to 29270863.json file.\n"
     ]
    }
   ],
   "source": [
    "# This code snippet adds a few more addons to the existing all_jobs data in terms of a dictionary that encapsualtes all the data \n",
    "# and places in the appropriate format as required.\n",
    "\n",
    "all_job_obj = {} #the dictionary that will hold the entire extracted data\n",
    "inner_obj = {} #dictionary, with a key to hold the all_job list data.\n",
    "inner_obj[\"listing\"] = all_jobs #assignement of extracted data to a \"listing\" key of inner_obj\n",
    "all_job_obj[\"listings\"] = inner_obj # assigning back to the all_job_obj dictionary, with \"listings\" as the key\n",
    "\n",
    "# writing the contents of the all_job_obj to 29270863.json\n",
    "fh = open(\"29270863.json\",\"w\")\n",
    "fh.write(json.dumps(all_job_obj))\n",
    "fh.close()\n",
    "\n",
    "print(\"The json content was written to 29270863.json file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Transforming the extracted Data to XML\n",
    "\n",
    "Now that we have transformed the raw text data to json, we have to do the same to convert the 'all_jobs' list to an xml format and write to a file. since there are no functions available to do this task for us, we have to write the logic to convert the list of dictionaries to xml string and write to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The xml content was written to 29270863.xml file.\n"
     ]
    }
   ],
   "source": [
    "#This code snippet basically iterates over all_jobs list, extracts information and puts it in an XML format.\n",
    "\n",
    "\n",
    "xml_str = \"<listings>\" #initializing a string variable with the beginning tag of the target XML document\n",
    "for each_job in all_jobs: #iterating through all the jobs \n",
    "    for k in each_job: #checking for ID key\n",
    "        if k == \"_id\":\n",
    "            sub_xml = \"<listing id = \"+'\"'+each_job[k]+'\"'\">\" # if so, then adding the ID in listing tag in required style.\n",
    "            xml_str += sub_xml\n",
    "        elif k == \"job_descriptions\" or k == \"job_responsibilities\" or k == \"required_qualifications\": #checking for keys which \n",
    "                                                                    #have dictionaries as values.\n",
    "            if type(each_job[k]) == dict:  #if true, an inner_xml string is created that concatenates the list items surrounded\n",
    "                                           # by appropriate tags and is in turn concatenated with the global string xml_str\n",
    "                inner_xml = \"\"\n",
    "                for key in each_job[k]:\n",
    "                    for each_item in each_job[k][key]:\n",
    "                        inner_xml += \"<\"+key+\">\"+each_item+\"</\"+key+\">\"\n",
    "                sub_xml = \"<\"+k+\">\"+inner_xml+\"</\"+k+\">\"\n",
    "                xml_str += sub_xml\n",
    "            else:\n",
    "                sub_xml = \"<\"+k+\">\"+each_job[k]+\"</\"+k+\">\"\n",
    "                xml_str += sub_xml\n",
    "        else:\n",
    "            sub_xml = \"<\"+k+\">\"+each_job[k]+\"</\"+k+\">\" # For other keys, the xml string is dynamically created and the data associated\n",
    "                                                        # is fetched accordingly and concatenated in the right place.\n",
    "            xml_str += sub_xml\n",
    "    xml_str += \"</listing>\" #the tag that marks the end of a job posting is terminated with </listing> tag\n",
    "xml_str += \"</listings>\" #once all the job postings are iterated over, the </listings> tag is appended to the end of the xml_str\n",
    "\n",
    "#writing the xml string to 29270863.xml file \n",
    "fh = open(\"29270863.xml\",\"w\")\n",
    "fh.write(xml_str)\n",
    "fh.close()\n",
    "\n",
    "print(\"The xml content was written to 29270863.xml file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Section 1. Demonstrated the importing of essential libraries like re and json to carry out Data parsing.\n",
    "\n",
    "##### Section 2. Showcased the Data parsing steps as follows:\n",
    "<ul>\n",
    "    <li><b>Section 2.1</b> This section deals with reading the contents of 29270863.dat file.</li>\n",
    "    <li><b>Section 2.2</b> This section deals with splitting the contents of the file by a delimiter. thereby attaining the contents of each file as a list item. This step is necessary to carry out further processing.</li>\n",
    "    <li><b>Section 2.3</b> This section explains in detail the way we arrived at our final regular expression which was used to extract keys/fields of each job posting</li>\n",
    "    <li><b>Section 2.4</b> This section handles the standardization of all the keys and capturing data of respective keys and persisting them in a dictionary where each dictionary represents a job posting and each dictionary will have a set of keys and values, where keys represent the keys/fields of the job posting and value represents its corresponding data. This eventually will we stored in one single list</li>\n",
    "    <li><b>Section 2.5</b> We perform certain post standardization analysis of the standardized keys/fields and check the number of files these occur in. That is followed by converting the list of dictionaries of job posting to appropriate format and then dumping it using the json package and eventually wrirting it to a 29270863.json file</li>\n",
    "    <li><b>Section 2.6</b> Lastly we use the List of dictionaries to create an XML string and the same is subsequently written to 29270863.xml file</li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "\n",
    "#### <li>https://docs.python.org/3/library/re.html</li>\n",
    "\n",
    "\n",
    "#### <li>https://docs.python.org/3/library/json.html</li>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
